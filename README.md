# Chess-with-MCTS

The many possible move combinations of chess and its outcomes cannot be computed and tried out manually. Thus a reinforcement-based learning approach needs 
to be employed. The agent learns to achieve a goal in an uncertain environment with a multitude of states like an agent playing chess in a chessboard. 
Monte Carlo Tree Search (MCTS) is used in in such cases as it scours search space and predicts the moves having high win rates as explorations occur. 
In this project, the said approach of reinforcement learning based on MCTS is undertaken to simulate a game of chess that progresses by calculating the 
best move every single time and displaying the chessboard and the moves taken on a GUI created using Streamlit.


### Contributors:  
[**Amrita Varshini E R**](https://github.com/amvarsh/)  
[**Ann Maria John**](https://github.com/anuachu1128/)  
[**Arunima Divya**](https://github.com/arunimadivya/)   
[**Devi Parvathy Nair**](https://github.com/DeviParvathyNair/)  
[**Namitha S**](https://github.com/namithas123/)  
